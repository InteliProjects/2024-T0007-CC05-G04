{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import de bibliotecas e opção de display de colunas'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUtilizar as seguintes tabelas: associacaoProdutoElo, blendagemProduto, clienteAPS, fornecedorAPS, patioAPS, \\nprevisaoAgregada, portoAPS, pesoMedioTransporteProduto, produtoAPS, submodalTransporte, trechoTransporte,\\nusinaBeneficiamentoAPS, usinaBriqueteAPS, usinaPelotizacaoAPS\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Utilizar as seguintes tabelas: associacaoProdutoElo, blendagemProduto, clienteAPS, fornecedorAPS, patioAPS, \n",
    "previsaoAgregada, portoAPS, pesoMedioTransporteProduto, produtoAPS, submodalTransporte, trechoTransporte,\n",
    "usinaBeneficiamentoAPS, usinaBriqueteAPS, usinaPelotizacaoAPS\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Realiza a leitura dos dados em formato CSV.\n",
    "\n",
    "Parâmetros:\n",
    "- Caminhos dos arquivos CSV contendo os dados a serem lidos.\n",
    "\n",
    "Retorno:\n",
    "- DataFrames contendo os dados lidos do arquivo CSV.\n",
    "'''\n",
    "clientes_aps = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\clienteAPS.csv\")\n",
    "fornecedor_aps = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\fornecedorAPS.csv\")\n",
    "portos_aps = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\portoAPS.csv\")\n",
    "usina_beneficiamento_aps = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\usinaBeneficiamentoAPS.csv\")\n",
    "usinas_briquete_aps = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\usinaBriqueteAPS.csv\")\n",
    "usinas_pelotizacao_aps = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\usinaPelotizacaoAPS.csv\")\n",
    "patios_aps = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\patioAPS.csv\")\n",
    "trechos_transporte = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\trechoTransporte.csv\")\n",
    "peso_medio = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\pesoMedioTransporteProduto.csv')\n",
    "produto_aps = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\produtoAPS.csv')\n",
    "previ_agregada = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\previsaoAgregada.csv\")\n",
    "prod_elo = pd.read_csv(r\"C:\\Users\\pedro\\Desktop\\trataDados\\csvs\\associacaoProdutoElo.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reduz a tabela de produtoElo e remove valores NaN.\n",
    "\n",
    "Parâmetros:\n",
    "- prod_elo: DataFrame contendo os codigos dos produtos e idEloCadeiaProducao.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame reduzido contendo apenas as colunas 'codigoProduto' e 'idEloCadeiaProducao' e sem valores NaN.\n",
    "'''\n",
    "prod_elo = prod_elo[[\"codigoProduto\", \"idEloCadeiaProducao\"]]\n",
    "prod_elo = prod_elo.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Agrupa os produtos por idEloCadeiaProducao.\n",
    "\n",
    "Parâmetros:\n",
    "- prod_elo: DataFrame contendo os dados dos produtos e idEloCadeiaProducao.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame agrupado por idEloCadeiaProducao, com a lista de códigos de produtos associados a cada id.\n",
    "'''\n",
    "prod_elo = prod_elo.groupby('idEloCadeiaProducao')['codigoProduto'].agg(list).reset_index()\n",
    "prod_elo[\"id\"] = prod_elo['idEloCadeiaProducao']\n",
    "prod_elo = prod_elo.drop(\"idEloCadeiaProducao\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv(filepath):\n",
    "    \"\"\"\n",
    "    Carrega um arquivo CSV em um DataFrame do pandas.\n",
    "\n",
    "    Parâmetros:\n",
    "    - filepath: Caminho do arquivo CSV a ser carregado.\n",
    "\n",
    "    Retorno:\n",
    "    - DataFrame contendo os dados do arquivo CSV.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def preprocess_dataframes():\n",
    "    \"\"\"\n",
    "    Pré-processa os dataframes individuais.\n",
    "\n",
    "    Retorno:\n",
    "    - Lista de dataframes contendo as colunas relevantes selecionadas.\n",
    "    - Dataframes individuais para cada categoria: clientes_aps, fornecedor_aps, portos_aps, usina_beneficiamento_aps, usinas_briquete_aps, usinas_pelotizacao_aps, patios_aps.\n",
    "    \"\"\"\n",
    "    dfs = [\n",
    "        clientes_aps[[\"id\", \"descricao\"]],\n",
    "        fornecedor_aps[[\"id\", \"descricao\"]],\n",
    "        portos_aps[[\"id\", \"descricao\", \"capacidadeMaximaEstocagem\"]],\n",
    "        usina_beneficiamento_aps[[\"id\", \"descricao\"]],\n",
    "        usinas_briquete_aps[[\"id\", \"descricao\"]],\n",
    "        usinas_pelotizacao_aps[[\"id\", \"descricao\"]],\n",
    "        patios_aps[[\"id\", \"descricao\", \"capacidadeMaximaEstocagem\"]],\n",
    "    ]\n",
    "\n",
    "    return dfs, clientes_aps, fornecedor_aps, portos_aps, usina_beneficiamento_aps, usinas_briquete_aps, usinas_pelotizacao_aps, patios_aps\n",
    "\n",
    "def merge_and_categorize(dfs, clientes_aps, fornecedor_aps, portos_aps, usina_beneficiamento_aps, usinas_briquete_aps, usinas_pelotizacao_aps, patios_aps):\n",
    "    \"\"\"\n",
    "    Mescla e categoriza os dataframes.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dfs: Lista de dataframes contendo as colunas relevantes selecionadas.\n",
    "    - Dataframes individuais para cada categoria: clientes_aps, fornecedor_aps, portos_aps, usina_beneficiamento_aps, usinas_briquete_aps, usinas_pelotizacao_aps, patios_aps.\n",
    "\n",
    "    Retorno:\n",
    "    - DataFrame resultante da mescla e categorização dos dataframes.\n",
    "    \"\"\"\n",
    "    node_data = pd.concat(dfs).groupby('id', as_index=False).first()\n",
    "\n",
    "    node_data[\"categorizacao\"] = None\n",
    "    categorization_conditions = [\n",
    "        node_data['id'].isin(clientes_aps['id']),\n",
    "        node_data['id'].isin(fornecedor_aps['id']),\n",
    "        node_data['id'].isin(portos_aps['id']),\n",
    "        node_data['id'].isin(usina_beneficiamento_aps['id']),\n",
    "        node_data['id'].isin(usinas_briquete_aps['id']),\n",
    "        node_data['id'].isin(usinas_pelotizacao_aps['id']),\n",
    "        node_data['id'].isin(patios_aps['id']),\n",
    "    ]\n",
    "\n",
    "    for condition, category in zip(categorization_conditions, [\"cliente\", \"fornecedor\", \"porto\", \"beneficiamento\", \"briquete\", \"pelotizacao\", \"patio\"]):\n",
    "        node_data.loc[condition, \"categorizacao\"] = category\n",
    "\n",
    "    return node_data\n",
    "\n",
    "def pipeline():\n",
    "    \"\"\"\n",
    "    Executa o pipeline de pré-processamento.\n",
    "\n",
    "    Retorno:\n",
    "    - DataFrame final após o pré-processamento.\n",
    "    \"\"\"\n",
    "    dfs, clientes_aps, fornecedor_aps, portos_aps, usina_beneficiamento_aps, usinas_briquete_aps, usinas_pelotizacao_aps, patios_aps = preprocess_dataframes()\n",
    "    node_data = merge_and_categorize(dfs, clientes_aps, fornecedor_aps, portos_aps, usina_beneficiamento_aps, usinas_briquete_aps, usinas_pelotizacao_aps, patios_aps)\n",
    "    return node_data\n",
    "\n",
    "# Executa o pipeline\n",
    "node_data = pipeline()\n",
    "\n",
    "# Exibe o dataframe final\n",
    "node_data = node_data.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Realiza a junção entre o dataframe node_data e o dataframe prod_elo utilizando a coluna 'id' como chave de junção.\n",
    "\n",
    "Parâmetros:\n",
    "- node_data: DataFrame contendo os dados principais a serem combinados.\n",
    "- prod_elo: DataFrame contendo os dados adicionais a serem adicionados ao node_data.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame resultante da junção entre node_data e prod_elo.\n",
    "\"\"\"\n",
    "node_data = pd.merge(node_data, prod_elo, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seleciona as colunas necessárias do dataframe trechos_transporte.\n",
    "\n",
    "Parâmetros:\n",
    "- trechos_transporte: DataFrame contendo os dados dos trechos de transporte.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo apenas as colunas 'idEloCadeiaProducaoDestino', 'idEloCadeiaProducaoOrigem' e 'codigoSubModal'.\n",
    "'''\n",
    "trechos_transporte = trechos_transporte[['idEloCadeiaProducaoDestino', 'idEloCadeiaProducaoOrigem', 'codigoSubModal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Realiza a categorização das rotas de transporte.\n",
    "\n",
    "Parâmetros:\n",
    "- trechos_transporte: DataFrame contendo os dados dos trechos de transporte.\n",
    "- node_data: DataFrame contendo os dados dos nós.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame categorizado das rotas de transporte.\n",
    "'''\n",
    "# Realiza o merge/junção para a categorização de origem\n",
    "rotas_categorizadas = pd.merge(trechos_transporte, node_data, left_on='idEloCadeiaProducaoOrigem', right_on='id', how='left')\n",
    "rotas_categorizadas.rename(columns={'categorizacao': 'categorizacaoOrigem', 'capacidadeMaximaEstocagem': 'capacidadeArmMaxOrigem', 'codigoProduto': 'prodOrigem'}, inplace=True)\n",
    "\n",
    "# Realiza o merge/junção para a categorização de destino\n",
    "rotas_categorizadas = pd.merge(rotas_categorizadas, node_data, left_on='idEloCadeiaProducaoDestino', right_on='id', how='left', suffixes=('', 'Destino'))\n",
    "rotas_categorizadas.rename(columns={'categorizacao': 'categorizacaoDestino', 'capacidadeMaximaEstocagem': 'capacidadeArmMaxDestino', 'codigoProduto': 'prodDestino'}, inplace=True)\n",
    "\n",
    "# Remove as colunas 'id' duplicadas que foram adicionadas durante o merge\n",
    "rotas_categorizadas.drop(columns=['id', 'idDestino'], inplace=True)\n",
    "\n",
    "rotas_categorizadas = rotas_categorizadas[[\"idEloCadeiaProducaoOrigem\", \"descricao\", \"categorizacaoOrigem\", \"capacidadeArmMaxOrigem\", \"prodOrigem\", \"idEloCadeiaProducaoDestino\", \"descricaoDestino\", \"categorizacaoDestino\", \"capacidadeArmMaxDestino\", \"prodDestino\", \"codigoSubModal\"]]\n",
    "rotas_categorizadas = rotas_categorizadas.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "produto_desejado = \"AF08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seleciona as colunas necessárias do dataframe peso_medio e filtra os dados pelo produto desejado.\n",
    "\n",
    "Parâmetros:\n",
    "- peso_medio: DataFrame contendo os dados do peso médio de transporte por produto.\n",
    "- produto_desejado: Código do produto desejado para filtragem.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo apenas as colunas 'codigoSubModal', 'pesoMedio' e 'codigoProduto' filtrado pelo produto desejado.\n",
    "'''\n",
    "peso_medio = peso_medio[[\"codigoSubModal\", \"pesoMedio\", \"codigoProduto\"]] # SFHG SSFG AF08\n",
    "peso_medio = peso_medio[peso_medio[\"codigoProduto\"] == produto_desejado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calcula a média dos pesos médios de transporte por produto e submodalidade.\n",
    "\n",
    "Parâmetros:\n",
    "- peso_medio: DataFrame contendo os dados do peso médio de transporte por produto e submodalidade.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo a média dos pesos médios de transporte por produto e submodalidade, arredondada para 2 casas decimais.\n",
    "'''\n",
    "media_pesos = peso_medio.groupby(['codigoProduto', 'codigoSubModal'])['pesoMedio'].mean().round(2).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filtra as rotas categorizadas removendo aquelas que têm o produto desejado como origem.\n",
    "\n",
    "Parâmetros:\n",
    "- rotas_categorizadas: DataFrame contendo as rotas categorizadas.\n",
    "- produto_desejado: Código do produto desejado para filtragem.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo as rotas categorizadas sem o produto desejado como origem.\n",
    "'''\n",
    "rotas_categorizadas = rotas_categorizadas[rotas_categorizadas[\"prodOrigem\"] != produto_desejado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Realiza a junção dos dados das rotas categorizadas com as médias dos pesos médios de transporte por submodalidade.\n",
    "\n",
    "Parâmetros:\n",
    "- rotas_categorizadas: DataFrame contendo as rotas categorizadas.\n",
    "- media_pesos: DataFrame contendo as médias dos pesos médios de transporte por produto e submodalidade.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo os dados tratados após a junção e remoção de linhas com valores nulos, reindexado.\n",
    "'''\n",
    "dados_tratados = pd.merge(rotas_categorizadas, media_pesos, on=\"codigoSubModal\", suffixes=('_original', '_media'))\n",
    "dados_tratados = dados_tratados.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filtra os dados tratados mantendo apenas as linhas em que o produto está presente tanto na origem quanto no destino.\n",
    "\n",
    "Parâmetros:\n",
    "- dados_tratados: DataFrame contendo os dados tratados após a junção.\n",
    "- produto_desejado: Código do produto desejado para filtragem.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo apenas as linhas em que o produto está presente tanto na origem quanto no destino.\n",
    "'''\n",
    "df_filtrado = dados_tratados[dados_tratados.apply(lambda row: (row['codigoProduto'] in row['prodOrigem']) and (row['codigoProduto'] in row['prodDestino']), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filtra os dados de previsão agregada removendo as linhas em que a quantidade de produto é igual a zero.\n",
    "\n",
    "Parâmetros:\n",
    "- previ_agregada: DataFrame contendo os dados de previsão agregada.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo os dados de previsão agregada com quantidade de produto diferente de zero.\n",
    "'''\n",
    "previsao_agregada = previ_agregada[previ_agregada[\"qtdeProduto\"] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filtra os dados de previsão agregada para demanda do tipo \"F\", identifica os períodos de maior frequência de início e fim,\n",
    "e ajusta as colunas de data para formato datetime e adiciona colunas de dia de início e fim.\n",
    "\n",
    "Parâmetros:\n",
    "- previ_agregada: DataFrame contendo os dados de previsão agregada.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo os dados de previsão agregada filtrados para demanda do tipo \"F\" e ajustado com colunas de data e dia de início e fim.\n",
    "'''\n",
    "demanda_f = previ_agregada[previ_agregada[\"tipoDemanda\"] == \"F\"]\n",
    "high = demanda_f[\"dataInicio\"].value_counts()\n",
    "demanda_f = demanda_f[demanda_f[\"dataInicio\"] == high.idxmax()]\n",
    "high2 = demanda_f[\"dataFim\"].value_counts()\n",
    "demanda_f = demanda_f[demanda_f[\"dataFim\"] == high2.idxmax()]\n",
    "demanda_f['dataInicio'] = pd.to_datetime(demanda_f['dataInicio'])\n",
    "demanda_f['dataFim'] = pd.to_datetime(demanda_f['dataFim'])\n",
    "demanda_f['diaInicio'] = demanda_f['dataInicio'].dt.day\n",
    "demanda_f['diaFim'] = demanda_f['dataFim'].dt.day\n",
    "demanda_f = demanda_f.drop_duplicates(subset=['codigoElo', 'codigoProduto'], keep=False).reset_index(drop=True)\n",
    "demanda_f = demanda_f[[\"codigoElo\", \"codigoProduto\", \"diaFim\", \"diaInicio\", \"qtdeProduto\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filtra os dados de previsão agregada para demanda do tipo \"V\", identifica os períodos de maior frequência de início e fim,\n",
    "e ajusta as colunas de data para formato datetime e adiciona colunas de dia de início e fim.\n",
    "\n",
    "Parâmetros:\n",
    "- previ_agregada: DataFrame contendo os dados de previsão agregada.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame contendo os dados de previsão agregada filtrados para demanda do tipo \"V\" e ajustado com colunas de data e dia de início e fim.\n",
    "'''\n",
    "demanda_v = previ_agregada[previ_agregada[\"tipoDemanda\"] == \"V\"]\n",
    "high = demanda_v[\"dataInicio\"].value_counts()\n",
    "demanda_v = demanda_v[demanda_v[\"dataInicio\"] == high.idxmax()]\n",
    "high2 = demanda_v[\"dataFim\"].value_counts()\n",
    "demanda_v = demanda_v[demanda_v[\"dataFim\"] == high2.idxmax()]\n",
    "demanda_v['dataInicio'] = pd.to_datetime(demanda_v['dataInicio'])\n",
    "demanda_v['dataFim'] = pd.to_datetime(demanda_v['dataFim'])\n",
    "demanda_v['diaInicio'] = demanda_v['dataInicio'].dt.day\n",
    "demanda_v['diaFim'] = demanda_v['dataFim'].dt.day\n",
    "demanda_v = demanda_v.drop_duplicates(subset=['codigoElo', 'codigoProduto'], keep=False).reset_index(drop=True)\n",
    "demanda_v = demanda_v[[\"codigoElo\", \"codigoProduto\", \"diaFim\", \"diaInicio\", \"qtdeProduto\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Concatena os DataFrames de demanda do tipo \"F\" e \"V\" e filtra os dados para o produto desejado.\n",
    "\n",
    "Parâmetros:\n",
    "- demanda_f: DataFrame contendo os dados de demanda do tipo \"F\".\n",
    "- demanda_v: DataFrame contendo os dados de demanda do tipo \"V\".\n",
    "- produto_desejado: Código do produto desejado para filtrar os dados.\n",
    "\n",
    "Retorno:\n",
    "- DataFrame resultante da concatenação das demandas e filtragem para o produto desejado.\n",
    "'''\n",
    "demandas = pd.concat([demanda_f, demanda_v]).reset_index(drop=True)\n",
    "demandas = demandas[demandas[\"codigoProduto\"] == produto_desejado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Realiza a junção (merge) de dois dataframes e aplica filtros aos dados resultantes.\n",
    "\n",
    "Parâmetros:\n",
    "- df_filtrado (pandas.DataFrame): DataFrame a ser mesclado à esquerda.\n",
    "- demandas (pandas.DataFrame): DataFrame a ser mesclado à direita.\n",
    "- produto_desejado (str): Código do produto desejado para filtrar os dados.\n",
    "\n",
    "Retorno:\n",
    "- pandas.DataFrame: DataFrame resultante da junção e aplicação de filtros.\n",
    "\"\"\"\n",
    "df_final = pd.merge(df_filtrado, demandas, left_on='idEloCadeiaProducaoDestino', right_on=\"codigoElo\", how='left')\n",
    "df_final.drop([\"codigoProduto_x\"], axis=1, inplace=True)\n",
    "df_final = df_final[(df_final['categorizacaoDestino'] != 'cliente') | (df_final['qtdeProduto'].notna())]\n",
    "df_final = df_final[df_final[\"qtdeProduto\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove colunas específicas do dataframe df_final e salva os dados restantes em um arquivo CSV.\n",
    "\n",
    "Parâmetros:\n",
    "- df_final (pandas.DataFrame): DataFrame contendo os dados a serem processados e salvos.\n",
    "- colunas_para_remover (list): Lista de nomes das colunas a serem removidas.\n",
    "\n",
    "\n",
    "Retorno:\n",
    "- Dataframe com as colunas indicadas removidas.\n",
    "\"\"\"\n",
    "df_final = df_final.drop([\"prodOrigem\", \"prodDestino\", \"codigoProduto_y\", \"codigoElo\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('dadosTratados.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Teste de Neo4j</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lê dados de um arquivo CSV tratado, cria consultas Cypher para um banco de dados de grafos e as escreve em um arquivo de texto.\n",
    "\n",
    "Parâmetros:\n",
    "- arquivo_csv (str): Nome do arquivo CSV tratado contendo os dados a serem processados.\n",
    "- arquivo_cypher (str): Nome do arquivo de texto onde as consultas Cypher serão escritas.\n",
    "\n",
    "Retorno:\n",
    "- None\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "# Abre o arquivo CSV para leitura\n",
    "with open('dadosTratados.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "\n",
    "    cypher_queries = []\n",
    "\n",
    "    for row in reader:\n",
    "        # Acessa os valores necessários da linha atual\n",
    "        id_origem = row['idEloCadeiaProducaoOrigem']\n",
    "        categorizacao_origem = row['categorizacaoOrigem']\n",
    "        descricao_origem = row['descricao']\n",
    "        id_destino = row['idEloCadeiaProducaoDestino']\n",
    "        categorizacao_destino = row['categorizacaoDestino']\n",
    "        descricao_destino = row['descricaoDestino']\n",
    "\n",
    "        # Adiciona consulta Cypher para criar ou mesclar nó de origem\n",
    "        cypher_queries.append(f\"MERGE (o:{categorizacao_origem} {{id: '{id_origem}', descricao: '{descricao_origem}'}})\")\n",
    "\n",
    "        # Adiciona consulta Cypher para criar ou mesclar nó de destino\n",
    "        cypher_queries.append(f\"MERGE (d:{categorizacao_destino} {{id: '{id_destino}', descricao: '{descricao_destino}'}})\")\n",
    "\n",
    "        # Adiciona consulta Cypher para criar relação entre origem e destino\n",
    "        cypher_queries.append(f\"MATCH (o:{categorizacao_origem} {{id: '{id_origem}'}}), (d:{categorizacao_destino} {{id: '{id_destino}'}}) MERGE (o)-[:CONNECTION]->(d)\")\n",
    "\n",
    "# Escreve as consultas em um arquivo de texto\n",
    "with open('cypher_queries.txt', 'w', encoding='utf-8') as file:\n",
    "    for query in cypher_queries:\n",
    "        file.write(query + ';\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
